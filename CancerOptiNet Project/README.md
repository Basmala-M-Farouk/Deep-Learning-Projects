# ðŸ§  CancerOptiNet  
**Deep Learning Optimization for Cancer Survival Prediction**

## ðŸ“˜ Project Description

This project applies deep learning techniques to a simulated global cancer patient dataset (2015â€“2024) to predict key outcomes such as **survival years**, **treatment cost**, and **severity score**. The main goal is to evaluate and compare the performance of various gradient-based optimizers (SGD, Adam, RMSprop, Adagrad) in terms of convergence speed, accuracy, and overall model performance.

---

## ðŸ©º Dataset Overview

The dataset includes global cancer data with the following features:

- **Demographics:** Age, Gender, Country/Region  
- **Medical Info:** Cancer Type, Stage, Severity Score  
- **Lifestyle & Risk Factors:** Smoking, Alcohol, Environmental Exposure  
- **Target Variables:**  
  - ðŸŽ¯ Survival Years  
  - ðŸ’° Treatment Cost  
  - ðŸš¨ Severity Score  

---

## ðŸ”§ Tasks Breakdown

### 1ï¸âƒ£ Data Preprocessing
- Normalized numeric features
- One-hot encoded categorical features
- Train-test split (80% training, 20% testing)

### 2ï¸âƒ£ Optimizer Comparison
Train the same neural network using different optimizers:
- ðŸ”¹ **Stochastic Gradient Descent (SGD)**
- ðŸ”¹ **Adam**
- ðŸ”¹ **RMSprop**
- ðŸ”¹ **Adagrad**

Each optimizer is evaluated for:
- Convergence behavior
- Learning rate sensitivity
- Stability during training

### 3ï¸âƒ£ Evaluation & Visualization
- Track and plot:
  - Training vs. Validation Loss
  - MAE / MSE (for regression)
  - Accuracy (if applicable)
- Final performance metrics per optimizer
- Discuss: Which optimizer performed best and why?

---

## ðŸ§  Learning Outcomes

### âœ… See Gradient Descent in Action
- Observe how each optimizer updates weights
- Compare their performance across metrics

### âœ… Understand Learning Rate & Momentum
- Explore the effects of learning rate tuning
- See how momentum and adaptive learning impact results

### âœ… Learn to Evaluate Optimizer Quality
- Spot overfitting, slow convergence, and poor generalization
- Understand optimizer choice in real-world problems

---

## ðŸ› ï¸ Tech Stack

- Python
- TensorFlow / Keras
- Scikit-learn
- Matplotlib & Seaborn
- Google Colab

---

## ðŸ“Š Results & Insights

> After training and comparing all models, the project concludes with a detailed analysis of each optimizerâ€™s behavior, convergence pattern, and generalization ability on real-world health data.

---

