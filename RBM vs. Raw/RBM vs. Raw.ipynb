{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tabulate import tabulate\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "hAv9TPCBq1nn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Data Preparation**\n",
        "\n",
        "– Scaling and Binarizing Fashion MNIST\n",
        "We start by loading the Fashion MNIST dataset and reshaping the 28x28 images into 784-dimensional vectors. To prepare the data for training, we normalize the pixel values to the range [0, 1] using MinMaxScaler. Since RBMs work best with binary input, we further binarize the data—converting all values above 0.5 to 1 and the rest to 0. This prepares the dataset for effective feature learning with the RBM."
      ],
      "metadata": {
        "id": "P6k11FjOr9RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and normalize Fashion MNIST\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype(np.float32)\n",
        "x_test = x_test.reshape(-1, 784).astype(np.float32)\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Binarize input for RBM\n",
        "x_train_bin = (x_train_scaled > 0.5).astype(np.float32)\n",
        "x_test_bin = (x_test_scaled > 0.5).astype(np.float32)"
      ],
      "metadata": {
        "id": "4ytMrvdVq_C7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Feature Extraction**\n",
        "\n",
        "– Training a Tuned RBM\n",
        "In this step, we train a Bernoulli Restricted Boltzmann Machine (RBM) to uncover hidden features from the binarized Fashion MNIST data. We use a well-tuned configuration with 512 hidden units, a moderate learning rate, and a batch size of 64, training for 30 iterations. Once trained, the RBM transforms both the training and testing datasets into compact, meaningful latent representations. These new feature vectors will later serve as input for our classification model."
      ],
      "metadata": {
        "id": "z3EuH0XqskGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Better Tuned RBM\n",
        "# -----------------------------\n",
        "rbm = BernoulliRBM(n_components=512, learning_rate=0.01, batch_size=64, n_iter=30, verbose=True, random_state=42)\n",
        "rbm.fit(x_train_bin)\n",
        "\n",
        "# Transform data\n",
        "x_train_rbm = rbm.transform(x_train_bin)\n",
        "x_test_rbm = rbm.transform(x_test_bin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEfrWjDRrDML",
        "outputId": "70a7e529-c1c0-403e-ba27-3415569c6713"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -213.72, time = 27.60s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -189.50, time = 23.17s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -170.41, time = 21.75s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -160.33, time = 25.82s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -148.88, time = 23.52s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -141.76, time = 23.83s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -137.36, time = 21.85s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -132.35, time = 23.78s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -129.14, time = 23.53s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -125.03, time = 24.54s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -122.12, time = 23.28s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -121.54, time = 22.22s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -118.90, time = 23.58s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -116.65, time = 23.92s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -114.04, time = 22.71s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -113.46, time = 22.25s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -110.82, time = 23.25s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -109.64, time = 23.72s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -109.57, time = 21.35s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -108.41, time = 22.78s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -106.89, time = 23.74s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -106.67, time = 24.05s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -105.65, time = 21.66s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -104.22, time = 22.67s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -103.80, time = 23.69s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -102.85, time = 21.47s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -103.13, time = 22.79s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -100.56, time = 23.90s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -101.10, time = 23.89s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -99.83, time = 21.46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Classification**\n",
        "\n",
        "– Training a Deep MLP on RBM Features\n",
        "Now that we have meaningful features from the RBM, we use them to train a deep Multi-Layer Perceptron (MLP) classifier. The model includes dense layers with batch normalization, ReLU activations, and dropout for regularization. This setup helps the network learn effectively while reducing overfitting. We train the MLP on the RBM-transformed training data for 25 epochs, using a 10% validation split to monitor performance. This classifier learns to map the compressed RBM features to the correct fashion category labels."
      ],
      "metadata": {
        "id": "yWjs3wJMttn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Deep MLP on RBM features\n",
        "# -----------------------------\n",
        "mlp_rbm = Sequential([\n",
        "    Input(shape=(512,)),\n",
        "    Dense(256), BatchNormalization(), tf.keras.layers.ReLU(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128), BatchNormalization(), tf.keras.layers.ReLU(),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "mlp_rbm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "mlp_rbm.fit(x_train_rbm, y_train, epochs=25, batch_size=128, validation_split=0.1, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TKSlFRnrGRM",
        "outputId": "27636f5f-e4ca-4e39-d351-8cb07dd71761"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6988 - loss: 0.8628 - val_accuracy: 0.8063 - val_loss: 0.5188\n",
            "Epoch 2/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.8030 - loss: 0.5431 - val_accuracy: 0.8207 - val_loss: 0.4806\n",
            "Epoch 3/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8141 - loss: 0.5062 - val_accuracy: 0.8240 - val_loss: 0.4612\n",
            "Epoch 4/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8234 - loss: 0.4796 - val_accuracy: 0.8297 - val_loss: 0.4625\n",
            "Epoch 5/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.8332 - loss: 0.4560 - val_accuracy: 0.8362 - val_loss: 0.4400\n",
            "Epoch 6/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8330 - loss: 0.4498 - val_accuracy: 0.8275 - val_loss: 0.4753\n",
            "Epoch 7/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8387 - loss: 0.4339 - val_accuracy: 0.8400 - val_loss: 0.4305\n",
            "Epoch 8/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.8388 - loss: 0.4280 - val_accuracy: 0.8348 - val_loss: 0.4406\n",
            "Epoch 9/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8488 - loss: 0.4088 - val_accuracy: 0.8368 - val_loss: 0.4451\n",
            "Epoch 10/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8463 - loss: 0.4093 - val_accuracy: 0.8432 - val_loss: 0.4235\n",
            "Epoch 11/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.8528 - loss: 0.3986 - val_accuracy: 0.8345 - val_loss: 0.4306\n",
            "Epoch 12/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8533 - loss: 0.3979 - val_accuracy: 0.8423 - val_loss: 0.4248\n",
            "Epoch 13/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8552 - loss: 0.3896 - val_accuracy: 0.8378 - val_loss: 0.4267\n",
            "Epoch 14/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.8567 - loss: 0.3825 - val_accuracy: 0.8432 - val_loss: 0.4393\n",
            "Epoch 15/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8554 - loss: 0.3822 - val_accuracy: 0.8367 - val_loss: 0.4462\n",
            "Epoch 16/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.8603 - loss: 0.3776 - val_accuracy: 0.8360 - val_loss: 0.4469\n",
            "Epoch 17/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.8621 - loss: 0.3708 - val_accuracy: 0.8422 - val_loss: 0.4299\n",
            "Epoch 18/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8611 - loss: 0.3673 - val_accuracy: 0.8462 - val_loss: 0.4233\n",
            "Epoch 19/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8628 - loss: 0.3606 - val_accuracy: 0.8415 - val_loss: 0.4192\n",
            "Epoch 20/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.8657 - loss: 0.3575 - val_accuracy: 0.8432 - val_loss: 0.4316\n",
            "Epoch 21/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.8680 - loss: 0.3523 - val_accuracy: 0.8448 - val_loss: 0.4234\n",
            "Epoch 22/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.8698 - loss: 0.3425 - val_accuracy: 0.8480 - val_loss: 0.4365\n",
            "Epoch 23/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8732 - loss: 0.3384 - val_accuracy: 0.8452 - val_loss: 0.4315\n",
            "Epoch 24/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8735 - loss: 0.3352 - val_accuracy: 0.8517 - val_loss: 0.4083\n",
            "Epoch 25/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.8749 - loss: 0.3322 - val_accuracy: 0.8448 - val_loss: 0.4379\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x791695bac090>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Baseline**\n",
        "\n",
        "– Training a Raw MLP Classifier\n",
        "To fairly evaluate the benefit of RBM-based feature extraction, we train a baseline MLP classifier directly on the raw, scaled pixel data (without RBM). This model uses a slightly larger architecture to match the complexity of the RBM-based MLP. We again apply batch normalization, ReLU activation, and dropout to improve learning and generalization. Training follows the same schedule—25 epochs with a batch size of 128 and 10% of data reserved for validation—allowing for a consistent comparison."
      ],
      "metadata": {
        "id": "_z-pQnMNueUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Raw MLP for comparison\n",
        "# -----------------------------\n",
        "mlp_raw = Sequential([\n",
        "    Input(shape=(784,)),\n",
        "    Dense(512), BatchNormalization(), tf.keras.layers.ReLU(),\n",
        "    Dropout(0.3),\n",
        "    Dense(256), BatchNormalization(), tf.keras.layers.ReLU(),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "mlp_raw.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "mlp_raw.fit(x_train_scaled, y_train, epochs=25, batch_size=128, validation_split=0.1, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2meOuEjdrIMt",
        "outputId": "6871cba9-297a-465f-9e78-a4bd0d7180b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.7684 - loss: 0.6601 - val_accuracy: 0.8373 - val_loss: 0.4379\n",
            "Epoch 2/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8544 - loss: 0.3961 - val_accuracy: 0.8637 - val_loss: 0.3659\n",
            "Epoch 3/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8750 - loss: 0.3481 - val_accuracy: 0.8673 - val_loss: 0.3540\n",
            "Epoch 4/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8840 - loss: 0.3143 - val_accuracy: 0.8782 - val_loss: 0.3290\n",
            "Epoch 5/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.8904 - loss: 0.2963 - val_accuracy: 0.8760 - val_loss: 0.3274\n",
            "Epoch 6/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.8923 - loss: 0.2886 - val_accuracy: 0.8747 - val_loss: 0.3369\n",
            "Epoch 7/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.8986 - loss: 0.2750 - val_accuracy: 0.8860 - val_loss: 0.3044\n",
            "Epoch 8/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.9012 - loss: 0.2660 - val_accuracy: 0.8765 - val_loss: 0.3245\n",
            "Epoch 9/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.9059 - loss: 0.2540 - val_accuracy: 0.8885 - val_loss: 0.3014\n",
            "Epoch 10/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9086 - loss: 0.2447 - val_accuracy: 0.8843 - val_loss: 0.3101\n",
            "Epoch 11/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9137 - loss: 0.2327 - val_accuracy: 0.8695 - val_loss: 0.3397\n",
            "Epoch 12/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.9153 - loss: 0.2242 - val_accuracy: 0.8893 - val_loss: 0.3170\n",
            "Epoch 13/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9160 - loss: 0.2190 - val_accuracy: 0.8897 - val_loss: 0.3160\n",
            "Epoch 14/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9182 - loss: 0.2136 - val_accuracy: 0.8870 - val_loss: 0.3145\n",
            "Epoch 15/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9230 - loss: 0.2043 - val_accuracy: 0.8850 - val_loss: 0.3100\n",
            "Epoch 16/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9258 - loss: 0.1955 - val_accuracy: 0.8653 - val_loss: 0.3790\n",
            "Epoch 17/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9247 - loss: 0.2004 - val_accuracy: 0.8745 - val_loss: 0.3537\n",
            "Epoch 18/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.9302 - loss: 0.1867 - val_accuracy: 0.8843 - val_loss: 0.3401\n",
            "Epoch 19/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9311 - loss: 0.1826 - val_accuracy: 0.8878 - val_loss: 0.3183\n",
            "Epoch 20/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.9341 - loss: 0.1756 - val_accuracy: 0.8875 - val_loss: 0.3765\n",
            "Epoch 21/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9371 - loss: 0.1704 - val_accuracy: 0.8853 - val_loss: 0.3440\n",
            "Epoch 22/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.9375 - loss: 0.1670 - val_accuracy: 0.8958 - val_loss: 0.3217\n",
            "Epoch 23/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9375 - loss: 0.1654 - val_accuracy: 0.8755 - val_loss: 0.3706\n",
            "Epoch 24/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9407 - loss: 0.1575 - val_accuracy: 0.8838 - val_loss: 0.3451\n",
            "Epoch 25/25\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.9442 - loss: 0.1491 - val_accuracy: 0.8933 - val_loss: 0.3287\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79177c0711d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Result Summary**\n",
        "\n",
        "– Tabulating Model Performance\n",
        "To clearly visualize and compare the effectiveness of both models, we present their evaluation metrics in a table. This includes the test loss and accuracy for:\n",
        "\n",
        "The MLP trained on RBM-extracted features\n",
        "\n",
        "The MLP trained on raw image data\n",
        "\n",
        "This summary provides a direct comparison to assess the impact of using unsupervised feature learning with an RBM on classification performance."
      ],
      "metadata": {
        "id": "BsdHehz_vIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Evaluation\n",
        "# -----------------------------\n",
        "rbm_preds = mlp_rbm.predict(x_test_rbm)\n",
        "rbm_loss = log_loss(y_test, rbm_preds)\n",
        "rbm_acc = accuracy_score(y_test, np.argmax(rbm_preds, axis=1))\n",
        "\n",
        "raw_preds = mlp_raw.predict(x_test_scaled)\n",
        "raw_loss = log_loss(y_test, raw_preds)\n",
        "raw_acc = accuracy_score(y_test, np.argmax(raw_preds, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxdpj08lrKUc",
        "outputId": "4d78b13d-1b20-498a-b711-2b2a3d02401b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Evaluation**\n",
        "\n",
        "– Comparing the Two Models\n",
        "Finally, we evaluate both classifiers on the test set to measure their performance. We calculate:\n",
        "\n",
        "Log Loss, which reflects how well the predicted probabilities match the true labels.\n",
        "\n",
        "Accuracy, indicating the proportion of correct predictions.\n",
        "\n",
        "The RBM-based MLP uses the hidden feature representation, while the raw MLP uses the original scaled pixel data. By comparing these metrics, we can see whether the RBM's unsupervised feature learning provides a meaningful boost to classification performance."
      ],
      "metadata": {
        "id": "esv8gaSxu0OU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TYvOeTnINWrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f5abe3-132e-418d-9918-2f81800e7df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═══════════════════════╤═════════════╤═════════════════╕\n",
            "│ Model                 │   Test Loss │ Test Accuracy   │\n",
            "╞═══════════════════════╪═════════════╪═════════════════╡\n",
            "│ MLP with RBM Features │      0.4354 │ 84.76%          │\n",
            "├───────────────────────┼─────────────┼─────────────────┤\n",
            "│ MLP with Raw Images   │      0.3432 │ 88.98%          │\n",
            "╘═══════════════════════╧═════════════╧═════════════════╛\n"
          ]
        }
      ],
      "source": [
        "# Comparison Table\n",
        "results = [\n",
        "    [\"Model\", \"Test Loss\", \"Test Accuracy\"],\n",
        "    [\"MLP with RBM Features\", f\"{rbm_loss:.4f}\", f\"{rbm_acc*100:.2f}%\"],\n",
        "    [\"MLP with Raw Images\", f\"{raw_loss:.4f}\", f\"{raw_acc*100:.2f}%\"]\n",
        "]\n",
        "\n",
        "print(tabulate(results, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n"
      ]
    }
  ]
}