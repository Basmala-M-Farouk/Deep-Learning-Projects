{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDkbJRxxBCod"
      },
      "source": [
        "**Setting Up the Environment for Object Detection**\n",
        "\n",
        "We installed Roboflow to manage datasets and Ultralytics to use YOLO models for object detection, setting up our environment for computer vision tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yCJucPOqWZG",
        "outputId": "ac5f88dd-9939-42cc-be15-208df2ec8b7a"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOcRgxgUBSlq"
      },
      "source": [
        "**Importing Libraries and Tools**\n",
        "\n",
        "We begin by importing all the essential libraries needed for our object detection project:\n",
        "\n",
        "Roboflow: To load and manage datasets from the Roboflow platform.\n",
        "\n",
        "os and glob: For handling file and directory operations.\n",
        "\n",
        "cv2 (OpenCV): For image processing and manipulation.\n",
        "\n",
        "matplotlib.pyplot: To visualize images and results.\n",
        "\n",
        "yaml: To read and write configuration files.\n",
        "\n",
        "ultralytics.YOLO: To use the powerful YOLO models for training and inference.\n",
        "\n",
        "shutil: To perform file operations like copying or moving files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTzRrJLqYPX",
        "outputId": "4ae95b03-92b5-477e-cde7-3d2be15d2d01"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "from ultralytics import YOLO\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcl-_31ABsfW"
      },
      "source": [
        "**Loading the Dataset from Roboflow**\n",
        "\n",
        "Next, we connect to Roboflow using our API key and access the desired dataset:\n",
        "\n",
        "We authenticate with the Roboflow API using our personal key.\n",
        "\n",
        "Then, we navigate to our workspace and select the specific Cityscapes project.\n",
        "\n",
        "We choose version 1 of the dataset and download it in the YOLOv8 format, ready for training with Ultralytics' YOLO models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oLsCtAVqgLp",
        "outputId": "7f96b6f5-e16a-47d9-fb61-47ff62339e2e"
      },
      "outputs": [],
      "source": [
        "rf = Roboflow(api_key=\"9FSdqSkLxd4ITsUw2uxG\")\n",
        "project = rf.workspace(\"luigiaworkspace\").project(\"cityscapes-zz0ur\")\n",
        "dataset = project.version(\"1\").download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymcftDlSCCTk"
      },
      "source": [
        "**Exploring the Dataset Structure**\n",
        "\n",
        "We now dive into the structure of our downloaded dataset:\n",
        "\n",
        "We define the base training folder and locate its images and labels subfolders.\n",
        "\n",
        "We list a few sample files from each folder to ensure the dataset is loaded correctly.\n",
        "\n",
        "We then search for image files (.jpg or .png) in the images folder.\n",
        "\n",
        "To proceed with analysis or visualization, we select the first image in the dataset.\n",
        "\n",
        "Based on the image filename, we construct the corresponding label file path from the labels folder by replacing the image extension with .txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4bOv0nAqnk4",
        "outputId": "57591470-e689-469b-c47f-8eb66bf1f00e"
      },
      "outputs": [],
      "source": [
        "# Base train folder\n",
        "train_folder = \"/content/Cityscapes-1/train\"\n",
        "\n",
        "# Images and labels folders\n",
        "images_folder = os.path.join(train_folder, \"images\")\n",
        "labels_folder = os.path.join(train_folder, \"labels\")\n",
        "\n",
        "print(\"Images folder files:\", os.listdir(images_folder)[:5])\n",
        "print(\"Labels folder files:\", os.listdir(labels_folder)[:5])\n",
        "\n",
        "# Find image files (.jpg or .png)\n",
        "image_files = glob.glob(images_folder + \"/*.jpg\") + glob.glob(images_folder + \"/*.png\")\n",
        "if not image_files:\n",
        "    raise FileNotFoundError(\"No image files found in:\", images_folder)\n",
        "\n",
        "# Pick first image\n",
        "image_path = image_files[0]\n",
        "\n",
        "# Corresponding label path in labels folder\n",
        "label_name = os.path.basename(image_path).replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")\n",
        "label_path = os.path.join(labels_folder, label_name)\n",
        "\n",
        "print(\"Selected image:\", image_path)\n",
        "print(\"Corresponding label:\", label_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxV3ul9zCONi"
      },
      "source": [
        "**Visualizing a Sample Image and Its Labels**\n",
        "\n",
        "With our dataset ready, we take a closer look at one of the training examples:\n",
        "\n",
        "We load the image using OpenCV and convert its color format from BGR to RGB for correct visualization with Matplotlib.\n",
        "\n",
        "The image is then displayed with a title, giving us a visual sense of what the model will learn from.\n",
        "\n",
        "We also open the corresponding YOLOv8 label file and print the first couple of lines to see how segmentation data is structured ‚Äî typically in the form of class IDs followed by normalized polygon coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "i1zGeuxyrfgD",
        "outputId": "d7d3cc99-8501-473a-8e17-f812b42d7d79"
      },
      "outputs": [],
      "source": [
        "# Load image and show\n",
        "img = cv2.imread(image_path)\n",
        "if img is None:\n",
        "    raise ValueError(\"Could not load image:\", image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.title(\"Sample Cityscapes Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Print first two lines of label\n",
        "with open(label_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    print(\"Sample YOLOv8 segmentation labels:\")\n",
        "    print(lines[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMGjwTfsEEDY"
      },
      "source": [
        "**Preparing and Customizing the Dataset Configuration**\n",
        "\n",
        "Before training our model, we configure the data.yaml file that tells YOLO how to interpret our dataset:\n",
        "\n",
        "Step 1: We create a backup of the original data.yaml (if it exists and hasn‚Äôt been backed up yet), ensuring we can revert changes if needed.\n",
        "\n",
        "Step 2: We read and print the contents of the original configuration from the backup to review its structure.\n",
        "\n",
        "Step 3: We define a new data.yaml file that specifies:\n",
        "\n",
        "The paths to the training and validation image directories.\n",
        "\n",
        "The number of object classes (nc: 3).\n",
        "\n",
        "The names of the classes: \"bicycle\", \"car\", and \"motorcycle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdNF--RKrghg",
        "outputId": "db59a41b-8410-4a84-b4fe-b7aa6548fd0b"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "dataset_path = \"/content/Cityscapes-1\"\n",
        "yaml_path = f\"{dataset_path}/data.yaml\"\n",
        "backup_yaml_path = f\"{dataset_path}/original_data.yaml\"\n",
        "\n",
        "# STEP 1: Backup original if it exists and not already backed up\n",
        "if os.path.exists(yaml_path) and not os.path.exists(backup_yaml_path):\n",
        "    shutil.copy(yaml_path, backup_yaml_path)\n",
        "    print(f\"üì¶ Backed up original data.yaml to {backup_yaml_path}\")\n",
        "\n",
        "# STEP 2: Read original (or backup if exists)\n",
        "if os.path.exists(backup_yaml_path):\n",
        "    with open(backup_yaml_path, \"r\") as f:\n",
        "        original_yaml = f.read()\n",
        "    print(\"üìÑ Original data.yaml (from backup):\\n\", original_yaml)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No original data.yaml backup found.\")\n",
        "\n",
        "# STEP 3: Define and write new data.yaml\n",
        "new_yaml = f\"\"\"\n",
        "train: {dataset_path}/train/images\n",
        "val: {dataset_path}/valid/images\n",
        "\n",
        "nc: 3\n",
        "names: [\"bicycle\", \"car\", \"motorcycle\"]\n",
        "\"\"\"\n",
        "\n",
        "with open(yaml_path, \"w\") as f:\n",
        "    f.write(new_yaml.strip())\n",
        "\n",
        "print(\"\\n‚úÖ New data.yaml written:\\n\", new_yaml.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQImjxNdEVTm"
      },
      "source": [
        "**Remapping Class Labels for YOLO**\n",
        "\n",
        "In this step, we ensure our label files align with the updated class definitions in data.yaml by remapping class IDs:\n",
        "\n",
        "We define a remap dictionary that translates the original class IDs (1, 2, 3) to the new IDs (0, 1, 2) used in training.\n",
        "\n",
        "We target the labels folders in both the train and validation datasets.\n",
        "\n",
        "For each label file:\n",
        "\n",
        "We read its content and process each line.\n",
        "\n",
        "If the line contains a valid class ID and polygon coordinates, we replace the original class ID with its new mapped value.\n",
        "\n",
        "The updated lines are written back to the file.\n",
        "\n",
        "This ensures consistency between the label files and the class names defined in data.yaml, preparing the dataset for error-free training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WzOFOm0xn7K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Class remapping: original_id -> new_id\n",
        "remap = {1: 0, 2: 1, 3: 2}\n",
        "\n",
        "# Folder paths\n",
        "label_dirs = [\n",
        "    \"/content/Cityscapes-1/train/labels\",\n",
        "    \"/content/Cityscapes-1/valid/labels\"\n",
        "]\n",
        "\n",
        "for label_dir in label_dirs:\n",
        "    for filename in os.listdir(label_dir):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "\n",
        "        path = os.path.join(label_dir, filename)\n",
        "        with open(path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if not parts or len(parts) < 3:\n",
        "                continue\n",
        "            try:\n",
        "                cls = int(parts[0])\n",
        "                if cls in remap and (len(parts) - 1) % 2 == 0:\n",
        "                    new_cls = remap[cls]\n",
        "                    new_line = \" \".join([str(new_cls)] + parts[1:]) + \"\\n\"\n",
        "                    new_lines.append(new_line)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        with open(path, \"w\") as f:\n",
        "            f.writelines(new_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-Ql893IG9s"
      },
      "source": [
        "**Training the YOLOv8 Segmentation Model**\n",
        "\n",
        "With everything in place, we launch the training process using Ultralytics' YOLOv8 segmentation model:\n",
        "\n",
        "We load a pretrained YOLOv8 model (yolov8m-seg.pt) as the starting point, benefiting from its pre-learned features.\n",
        "\n",
        "The training is configured using our customized data.yaml file, which defines the dataset structure and classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaoPHATqsPfE",
        "outputId": "93530d88-287c-41fe-d3d2-f952cebde28e"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the pretrained segmentation model\n",
        "model = YOLO(\"yolov8m-seg.pt\")  # You can also use yolov8m-seg.pt or others for better accuracy\n",
        "\n",
        "\n",
        "model.train(\n",
        "    data=\"/content/Cityscapes-1/data.yaml\",\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16,        # number of images per batch (increase if your GPU has enough memory)\n",
        "    device=0,        # GPU device index; 0 means first GPU; use 'cpu' if no GPU available\n",
        "    workers=4        # number of CPU workers for loading data (helps speed up data loading)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSK9Oxsp423u",
        "outputId": "d9f92317-48c5-48cc-8b0d-65868525fcf0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgMdzWXhMC6u"
      },
      "source": [
        "**Displaying a Test Video in the Notebook**\n",
        "\n",
        "After training, we may want to visualize results or test our model on video input. This cell does just that:\n",
        "\n",
        "We specify the path to a video file stored in Google Drive.\n",
        "\n",
        "The video is read in binary mode, encoded in Base64, and embedded directly into the notebook using HTML.\n",
        "\n",
        "This allows the video to be played back inline, without needing to download or open it externally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "VgpqfxRkK-kt",
        "outputId": "abf7b6bb-7d03-44a6-b67b-2a7e0dd5b58e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Update the filename if it's different\n",
        "video_path = \"/content/drive/MyDrive/Prof data/video_tst.mp4\"\n",
        "\n",
        "# Encode video to base64\n",
        "mp4 = open(video_path, 'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "# Display video\n",
        "HTML(f\"\"\"\n",
        "<video width=640 controls>\n",
        "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyvPeocTMR97"
      },
      "source": [
        "**Running Inference on a Video with the Trained YOLOv8 Model**\n",
        "\n",
        "Now that our model is trained, we use it to detect and segment objects in a video:\n",
        "\n",
        "Load the trained model from the best weights saved during training.\n",
        "\n",
        "Specify the input video to test the model‚Äôs performance in real-world scenarios.\n",
        "\n",
        "Run predictions using the YOLO model, saving the output video with segmentation overlays.\n",
        "\n",
        "Locate the output .avi file generated by YOLO.\n",
        "\n",
        "Convert the video to .mp4 using ffmpeg for browser compatibility and smoother playback.\n",
        "\n",
        "Read and Base64 encode the .mp4 file.\n",
        "\n",
        "Display the result inline using HTML so we can instantly watch how well our model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ibQa14T5JL1h",
        "outputId": "db090d1c-68ce-4d8d-ff0d-8568f342089a"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Step 1: Load your trained YOLO model\n",
        "model = YOLO(\"/content/runs/segment/train/weights/best.pt\")\n",
        "\n",
        "# Step 2: Path to the input video\n",
        "input_video = \"/content/drive/MyDrive/Prof data/video_tst.mp4\"\n",
        "\n",
        "# Step 3: Run prediction and save results\n",
        "results = model.predict(source=input_video, save=True, conf=0.4)\n",
        "\n",
        "# Step 4: Find the saved .avi video in the output directory\n",
        "output_dir = results[0].save_dir  # e.g. 'runs/segment/predict3'\n",
        "avi_files = glob.glob(os.path.join(output_dir, \"*.avi\"))\n",
        "if not avi_files:\n",
        "    raise FileNotFoundError(\"No AVI file found in output directory!\")\n",
        "\n",
        "avi_path = avi_files[0]\n",
        "mp4_path = avi_path.replace(\".avi\", \".mp4\")\n",
        "\n",
        "# Step 5: Convert .avi to .mp4 using ffmpeg\n",
        "!ffmpeg -y -i \"$avi_path\" -vcodec libx264 -crf 23 \"$mp4_path\"\n",
        "\n",
        "# Step 6: Read and encode the .mp4 video for inline display\n",
        "with open(mp4_path, 'rb') as f:\n",
        "    video_data = f.read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "# Step 7: Display video in notebook\n",
        "HTML(f\"\"\"\n",
        "<video width=640 controls>\n",
        "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "</video>\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
